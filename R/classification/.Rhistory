#Getting data
dataset <- output
dataset
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# Set random seed to make results reproducible:
set.seed(17)
# create a list of 80% of the rows in the original dataset we can use for training
indexes <- createDataPartition(dataset$minbestconf, p=0.85, list=FALSE)
# select 20% of the data for validation
training <- dataset[indexes,]
validation1 <- dataset[-indexes,]
# Perform training:
rf_classifier = randomForest(minbestconf ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_classifier
confusion.training <- rf_classifier$confusion
confusion.training
varImpPlot(rf_classifier)
# Validation set assessment #1: looking at confusion matrix
prediction_for_table <- predict(rf_classifier,validation1[,-4])
#Getting data
dataset <- output
dataset
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost"))]
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NW"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# Set random seed to make results reproducible:
set.seed(17)
# create a list of 80% of the rows in the original dataset we can use for training
indexes <- createDataPartition(dataset$minbestconf, p=0.85, list=FALSE)
# select 20% of the data for validation
training <- dataset[indexes,]
validation1 <- dataset[-indexes,]
# Perform training:
rf_classifier = randomForest(minbestconf ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_classifier
confusion.training <- rf_classifier$confusion
#Getting data
dataset <- output
dataset
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NW"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$minbestconf, p=0.70, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$minbestconf)
# summarize the class distribution
percentage <- prop.table(table(dataset$minbestconf)) * 100
cbind(freq=table(dataset$minbestconf), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
set.seed(7)
fit.lda <- caret::train(minbestconf~., data=dataset, method="lda", metric=metric, trControl=control)
#
set.seed(7)
fit.sda <- caret::train(minbestconf~., data=dataset, method="sda", metric=metric, trControl=control)
# a) linear algorithms
set.seed(7)
fit.slda <- caret::train(minbestconf~., data=dataset, method="slda", metric=metric, trControl=control)
#2 (pda) Penalized Discriminant Analysis
set.seed(7)
fit.pda <- caret::train(minbestconf~., data=dataset, method="pda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- caret::train(minbestconf~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- caret::train(minbestconf~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- caret::train(minbestconf~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- caret::train(minbestconf~., data=dataset, method="rf", metric=metric, trControl=control)
# AdaBoost
set.seed(7)
fit.gbm <- caret::train(minbestconf~., data=dataset, method="gbm", metric=metric, trControl=control)
# AdaBoost
set.seed(7)
fit.glm <- caret::train(minbestconf~., data=dataset, method="glm", metric=metric, trControl=control)
# AdaBoost
set.seed(7)
fit.glmboost <- caret::train(minbestconf~., data=dataset, method="glmboost", metric=metric, trControl=control)
# summarize accuracy of models
#results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf,gbm=fit.gbm,glm=fit.glm,glmboost=fit.glmboost))
results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
print(fit.rf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
#Getting data
dataset <- output
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NW"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# Set random seed to make results reproducible:
set.seed(17)
# create a list of 80% of the rows in the original dataset we can use for training
indexes <- createDataPartition(dataset$minbestconf, p=0.85, list=FALSE)
# select 20% of the data for validation
training <- dataset[indexes,]
validation1 <- dataset[-indexes,]
# Perform training:
rf_classifier = randomForest(minbestconf ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_classifier
confusion.training <- rf_classifier$confusion
confusion.training
varImpPlot(rf_classifier)
#Getting data
dataset <- output
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NW","balanced"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# Set random seed to make results reproducible:
set.seed(17)
# create a list of 80% of the rows in the original dataset we can use for training
indexes <- createDataPartition(dataset$minbestconf, p=0.85, list=FALSE)
# select 20% of the data for validation
training <- dataset[indexes,]
validation1 <- dataset[-indexes,]
# Perform training:
rf_classifier = randomForest(minbestconf ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_classifier
confusion.training <- rf_classifier$confusion
confusion.training
rf.cv <- rfcv(training, training$Species, cv.fold=10)
varImpPlot(rf_classifier)
#Getting data
dataset <- output
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NR","NW","balanced"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$minbestconf, p=0.70, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$minbestconf)
# summarize the class distribution
percentage <- prop.table(table(dataset$minbestconf)) * 100
cbind(freq=table(dataset$minbestconf), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
set.seed(7)
fit.lda <- caret::train(minbestconf~., data=dataset, method="lda", metric=metric, trControl=control)
#
set.seed(7)
fit.sda <- caret::train(minbestconf~., data=dataset, method="sda", metric=metric, trControl=control)
# a) linear algorithms
set.seed(7)
fit.slda <- caret::train(minbestconf~., data=dataset, method="slda", metric=metric, trControl=control)
#2 (pda) Penalized Discriminant Analysis
set.seed(7)
fit.pda <- caret::train(minbestconf~., data=dataset, method="pda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- caret::train(minbestconf~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- caret::train(minbestconf~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- caret::train(minbestconf~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- caret::train(minbestconf~., data=dataset, method="rf", metric=metric, trControl=control)
# summarize accuracy of models
#results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf,gbm=fit.gbm,glm=fit.glm,glmboost=fit.glmboost))
results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
print(fit.rf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
#Getting data
dataset <- output
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NW","balanced"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$minbestconf, p=0.70, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$minbestconf)
# summarize the class distribution
percentage <- prop.table(table(dataset$minbestconf)) * 100
cbind(freq=table(dataset$minbestconf), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
set.seed(7)
fit.lda <- caret::train(minbestconf~., data=dataset, method="lda", metric=metric, trControl=control)
#
set.seed(7)
fit.sda <- caret::train(minbestconf~., data=dataset, method="sda", metric=metric, trControl=control)
# a) linear algorithms
set.seed(7)
fit.slda <- caret::train(minbestconf~., data=dataset, method="slda", metric=metric, trControl=control)
#2 (pda) Penalized Discriminant Analysis
set.seed(7)
fit.pda <- caret::train(minbestconf~., data=dataset, method="pda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- caret::train(minbestconf~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- caret::train(minbestconf~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- caret::train(minbestconf~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- caret::train(minbestconf~., data=dataset, method="rf", metric=metric, trControl=control)
# summarize accuracy of models
#results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf,gbm=fit.gbm,glm=fit.glm,glmboost=fit.glmboost))
results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
print(fit.rf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.knn, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
#Getting data
dataset <- output
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NW"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$minbestconf, p=0.70, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$minbestconf)
# summarize the class distribution
percentage <- prop.table(table(dataset$minbestconf)) * 100
cbind(freq=table(dataset$minbestconf), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
set.seed(7)
fit.lda <- caret::train(minbestconf~., data=dataset, method="lda", metric=metric, trControl=control)
#
set.seed(7)
fit.sda <- caret::train(minbestconf~., data=dataset, method="sda", metric=metric, trControl=control)
# a) linear algorithms
set.seed(7)
fit.slda <- caret::train(minbestconf~., data=dataset, method="slda", metric=metric, trControl=control)
#2 (pda) Penalized Discriminant Analysis
set.seed(7)
fit.pda <- caret::train(minbestconf~., data=dataset, method="pda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- caret::train(minbestconf~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- caret::train(minbestconf~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- caret::train(minbestconf~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- caret::train(minbestconf~., data=dataset, method="rf", metric=metric, trControl=control)
# summarize accuracy of models
#results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf,gbm=fit.gbm,glm=fit.glm,glmboost=fit.glmboost))
results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.cart, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
#Getting data
dataset <- output
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NW"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$minbestconf, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$minbestconf)
# summarize the class distribution
percentage <- prop.table(table(dataset$minbestconf)) * 100
cbind(freq=table(dataset$minbestconf), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
set.seed(7)
fit.lda <- caret::train(minbestconf~., data=dataset, method="lda", metric=metric, trControl=control)
#
set.seed(7)
fit.sda <- caret::train(minbestconf~., data=dataset, method="sda", metric=metric, trControl=control)
# a) linear algorithms
set.seed(7)
fit.slda <- caret::train(minbestconf~., data=dataset, method="slda", metric=metric, trControl=control)
#2 (pda) Penalized Discriminant Analysis
set.seed(7)
fit.pda <- caret::train(minbestconf~., data=dataset, method="pda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- caret::train(minbestconf~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- caret::train(minbestconf~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- caret::train(minbestconf~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- caret::train(minbestconf~., data=dataset, method="rf", metric=metric, trControl=control)
# summarize accuracy of models
#results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf,gbm=fit.gbm,glm=fit.glm,glmboost=fit.glmboost))
results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
# summarize Best Model
print(fit.lda)
print(fit.gbm)
print(fit.pda)
print(fit.rf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.cart, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.knn, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
#Getting data
dataset <- output
dataset
dataset <- dataset[ , -which(names(dataset) %in% c("inst","minbestcost","typeD","typeF","NW"))]
dataset$minbestconf <- as.factor(dataset$minbestconf)
dataset$balanced <- as.factor(dataset$balanced)
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$minbestconf, p=0.85, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]
dataset
names(dataset)
# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$minbestconf)
# summarize the class distribution
percentage <- prop.table(table(dataset$minbestconf)) * 100
cbind(freq=table(dataset$minbestconf), percentage=percentage)
# summarize attribute distributions
summary(dataset)
# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
# a) linear algorithms
set.seed(7)
fit.lda <- caret::train(minbestconf~., data=dataset, method="lda", metric=metric, trControl=control)
#
set.seed(7)
fit.sda <- caret::train(minbestconf~., data=dataset, method="sda", metric=metric, trControl=control)
# a) linear algorithms
set.seed(7)
fit.slda <- caret::train(minbestconf~., data=dataset, method="slda", metric=metric, trControl=control)
#2 (pda) Penalized Discriminant Analysis
set.seed(7)
fit.pda <- caret::train(minbestconf~., data=dataset, method="pda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- caret::train(minbestconf~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- caret::train(minbestconf~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- caret::train(minbestconf~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- caret::train(minbestconf~., data=dataset, method="rf", metric=metric, trControl=control)
# summarize accuracy of models
#results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf,gbm=fit.gbm,glm=fit.glm,glmboost=fit.glmboost))
results <- resamples(list(lda=fit.lda,sda=fit.sda,slda=fit.slda,pda=fit.pda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
# compare accuracy of models
dotplot(results)
print(fit.rf)
# estimate skill of LDA on the validation dataset
predictions <- predict(fit.rf, validation)
length(predictions)
length(validation$minbestconf)
confusionMatrix(predictions, validation$minbestconf)
